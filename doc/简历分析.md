![[Pasted image 20250310085132.png]]


### 自我介绍：

面试官您好，我叫汪国帆，2021年毕业于华东师范大学的软件工程专业，毕业后一直在当前的公司，映驰科技，从事车载软件方向的工作；主要担任SOC平台软件开发，工作涉及 APA、中间件以及系统软件开发 多个方向。
此次前来是想应聘的岗位，这与本人过往的工作经历也比较契合，也感谢贵公司给予我面试的机会，谢谢。


#### 工作历程：
早期（前半年）主要负责SOC侧小功能的开发，比如一些采图工具、负载测试工具等等
中期（前2年）参与到泊车项目中，负责平台化工作，包括其中部分功能组件开发（相机、描画、拼接），整包中底软的维护（J3、TDA4），整体的软件集成
后期（近两年）中间件中的一些服务维护与开发，时间同步等其他产品线，AI模型的嵌入式移植和测试；主要由于缺少泊车量产项目，并且人员变动导致的其他业务线人员短缺（中间件、算法）


映驰介绍：以车载中间件为核心，延伸一些列应用和服务业务的车载软件公司
	* 中间件：主打确定性调度，基于SOA架构（SOME/IP），提供了一系列服务并封装，助力于车载软件的开发
	* SOA：面相服务架构
	* 应用：以自研的APA（已量产）HPA为主
	* 服务：为一些公司的业务场景提供软件解决方案（调度优化）

-----------------------以往工作部分
### 中间件服务开发和维护

首先这里提到的中间件，是一套为车载软件设计的，基于SOA架构的软件开发框架，可以让用户可视化的构建和生成代码，并通过一系列的中间件服务，提供多种功能接口，提高软件的开发效率。

本人主要参与了其中部分中间件服务的开发与维护工作

##### SOMEIP和SOA
注：***不熟悉，只可泛泛而谈***

SOA：
SOA（面向服务架构）是一种应用程序架构，在这种架构中，所有功能都定义为独立的**服务**，这些服务带有**定义明确的可调用接口**，能够以定义好的顺序调用这些服务来形成业务流程。
**服务：** 服务是一种比构件粒度更大的信息集合，实际是包含实现了多个关联业务需求的逻辑组合，并且允许每个服务使用特定的平台，架构或技术方案；
**可调用接口：** 面向服务的接口不同于构件的接口，他的实现与特定语言无关，与特定的平台也无关，可十分方便的实现不同异构平台的交互

SOME/IP：
详见 https://zhuanlan.zhihu.com/p/459169297
要点：
* 应用层协议，默认基于以太网，与autosar兼容
* 分为SOME/IP、SOME/IP-SD（用于服务发现）、SOME/IP-TP（用于拆包组包）
* 通信（RPC）提供三种方式：method、event、field
	* method：客户端 req，服务端 response 或不返回
	* event：订阅后 进行事件触发
	* field：类似共享变量，提供get、set、notifie
* 通信依赖序列化

优点：（相较于传统can）
* 以太网带宽大，应对soc高带宽需求
* 面相服务，节省带宽
* 面向服务，扩展性强
等等

明确：
- 首先需要明确的是SOME/IP不是SOA，SOA也不是SOME/IP；
- 由于SOME/IP本身也是一种面向服务的协议，所以一般认为SOME/IP只不过是一种实现SOA可行的协议选择；
- 一般而言，基于消息的通信与RPC(Remote Procedure Call 远程过程调用)通信都可以实现SOA，而SOME/IP就是一种基于RPC框架的协议；
- 可以通过SOME/IP用来实现SOA，但SOA的实现却不一定非得用SOME/IP;

#### 时钟服务：


> [!NOTE] linux中的几个时钟
> * 硬件时钟（RTC）：主板单独供电，硬件计时，下电仍计时，用于初始化系统时间，也可以被系统时间同步
> * 系统时钟（RealTime）：内核维护，全局系统时钟，可被修改，通常被ntp同步
> * 启动时钟（BootTime）：内核维护，系统启动完成后 从0计数的单调时钟；不被ntp、date等修改；系统休眠时暂停计数
> * 单调时钟/高精度时钟（MonotonicTime）：内核维护，内核初始化起 从0计数的单调时间；不被ntp、date等修改；系统休眠时仍然计数，可以提供纳秒级精度，通常用于计时
> * 网卡时钟：网卡供电并计时，用于高精度时间同步

##### 职责
完整参与开发与维护
##### 主要功能：
一组封装的接口集，通过接口，提供域内多个时钟的时间戳和时钟状态
时钟包括本地时钟（系统启动时钟，通常是boottime），域内时钟（域内时间同步所用的时钟，通常是网卡时钟），世界时钟（通常ntp同步后的系统时钟）
* ***重点是域内时钟***，为 高精度时间同步（也就是gptp）封装了应用级接口，让用户在不直接进行系统调用的情况下，获取到高精度同步时间，同时了解同步的一系列状态，包括：是否同步、是否跳变、同步精度等等
* 隔离系统接口

##### 技术点：
* 时钟访问，主要通过系统调用clock_gettime，访问不同时钟（fd），其中网卡时钟根据平台网卡设备适配
* 域内时钟状态，通过shm与gptp进程交互，gptp写，时钟服务仅读；读写粒度在8字节范畴，保证操作原子性；存在多个时钟服务实例的情况下，访问不冲突
* 根据业务调整，如：历史同步过，但当前未同步的情况，时间是否可用；已同步，但精度较差，时间是否可用


#### 日志服务：

##### 职责：
主要参与中期的开发和维护
负责日志落盘和存储部分：

##### 主要功能
 为应用提供封装好的日志接口，包括：
 * 添加日志冗余信息，包括：日志时间，所属组件名称，代码行数，线程id
* 提供三种输出方式：前台输出，本地存储，远程实时传输至上位机
* 提供日志等级，并根据运行时设置，选择性输出日志

##### 技术点：
* 异步存储和传输，实时日志用内存池管理，定时线程周期进行存储和传输
* 内存池使用循环数组+锁实现，由于每次输入数据不定长，使用双指针管理头尾
* 本地存储需要日志回滚，维护日志名队列，达到上限后清楚头部日志
* 首先日志服务支持单进程，多进程下会有多个实例
* 日志服务内部维护两个线程，分别用于：前台IO输出（终端），后台IO输出（文件或网络）
* 后台输出时，会维护一套metadata，主要记录动态的组件信息（组件ID，组件名称等）；
	文件输出时，会同时存储metadata、日志正文（新版本还会添加校验，可选）
	网络输出时，会传输同样的日志正文，同时提供rpc方法，让上位机能够获取到metadata
* metadata作用，上位机获取板端的日志服务配置，用于 module id 和 module name的匹配，使得上位机可以获得到使用日志的模块组件名称，并进行远程配置
* 校验的场景，日志可以用于内部的trace功能（通过中间件日志，统计调度情况）
* 日志除了正文，还包含时间、日志等级（用于筛选）、组件名称、代码行数（通过宏\_\_FILE\_\_和\_\_LINE\_\_实现 ）、TID/PID （排查挂死等问题很有用，因为调度的线程分布是随机的）

##### 摘要（避免遗忘代码）：

logManager 
-> logConfig：管理metadata
-> logRecord：
	-> displayout：前台  // 快速&实时性
	-> ioOut：后台  // 一定滞后性
	->

#### 通信服务： 

##### 主要功能：
中间件的主要服务，基于SOME/IP协议客制化开发，实现组件间的RPC通信（特点基本与someip一致）
> [!NOTE] SOMEIP客制化
> 引用了SOMEIP的架构，实际实现有区别：
> * 不一定基于 UDP 实现，可能基于 二层通信、SPI、IPCF等实现
> * 简化了协议规格，如简化了许多错误处理机制
> 所以不能与其他SOMEIP应用对接，仅供中间件内部使用
##### 职责：
***主要负责片间通信部分（SOC-MCU）的SOC侧底层实现和封装***
包括：
* 不同协议适配：自研二层通信、UDP、SPI、IPCF（共享内存）
* 不同SOC平台：linux、qnx
* 自定义报文，附加信息：
	* 协议号（针对二层）
	* 数据ID
	* 长度，拆包相关长度信息
	* 校验码
##### 技术点：
* 二层通信，使用raw socket，主要针对早期部分mcu无udp协议栈设计
* SPI、IPCF等简单协议，自定义拆包组包逻辑、丢包处理、丢包判断等

#### 持久化服务：
##### 主要功能
一组封装的接口集
负责管理文件落盘、文件管理、文件备份和恢复等（借鉴AP持久化服务需求）
* 可作用于泊车过程中标定数据存储、配置数据存储（大小固定，避免损坏）
* 文件系统隔离
##### 职责：
完整的开发和维护
##### 技术点：
* 基于C 的标准文件接口实现
* 通过持久化配置，管理文件，包括文件属性，落盘位置，文件上限，是否需要校验、备份（可靠存储相关）
* 可靠存储文件添加冗余信息，和备份存储

-------------------------------------------------------------

##### Aspice开发流程：
需求分析->系统设计（软件架构、详细架构）->软件实现->集成测试（代码检查、单元测试、集成测试）-> 维护及bug修复 -> 交付
关键是保证流程的可追溯，需求 --> 架构 --> 代码模块 -->测试用例 --> 变更/修复


### 软件工具链：
#### gptp时间同步

##### 主要功能
gptp是基于802.1AS标准定义的基于以太网的时间同步协议，是TSN协议族的一部分；
区别于ptp4l：
* 功能简化+客制化，效率更高
* ptp4l仅linux版本，早期需要mcu版本，后期qnx版本

##### 职责
负责SOC侧后期的开发和维护（前期预研版本由其他大佬实现）

##### 技术点：
* gptp原理，烂熟于心
* gptp系统依赖，主要是hardtimestamp支持，其次访问网卡，raw socket需要权限支持
* qnx下gptp实现：
	* 通信无法使用raw sock，只能使用bpf实现，bpf指令组包+write+read
	* 调度上没有epoll，使用pthread并发；epoll是现实，通epoll_ctl注册回调，并通过循环epoll_wait来等待处理数据包
	* 网卡不支持硬时间戳，只能用realtime代替
	* 不支持socket recvmsg，只能通过pcap来捕获收发包时的时间戳

##### gptp和ptp、ntp区别：
主要的就是精度不同，gptp最高；
其次gptp是ptp精细化并加工后的产物，gptp只支持ptp，而其他支持e2e；
gptp必须有硬件支持（我们的gptp应用支持软时间戳，但精度会受影响），需要支持硬时间戳的mac和支持gptp的交换机


#### 通信工具链：
以库的形式封装，可以用于中间件，也可以用于其他应用
具体内容详见通信服务


--------------------------------------------------------------------

### 泊车软件APA

#### APA概述
L2,自动泊车功能，通过高算力SOC和实时MCU组合实现
泊车流程：
	1. 传感器输入，包括：相机和超声雷达
	2. 数据前处理：图像格式转换、去畸变、拼接，最终生成符合模型输入的bev（俯视）图像数据
	3. 模型推理，主要模型：线库位检测、图像分割、障碍物检测
	4. 数据后处理：结合底盘数据，跟踪并输出库位和障碍物信息
	5. 路径规划：规划出合理的泊车路径
	6. 握手&路径跟踪（mcu侧）：路径转化为can信号输出给底盘，控车完成泊车

量产上汽大通、合众哪吒，poc很多
涉及多平台：地平线（J3、J5、J6）、Ti（TDA4）（注：唯二的量产平台）、高通（8155）、ORIN，系统设计linux、android、qnx系统

##### 平台特性：
* qnx系统特殊性：
	* 系统环境近似但不同于linux，包括：编译选项、系统命令、系统库函数的支持（以网络为例，支持udp/tcp socket 但不支持raw socket）
	* 具备功能安全认真，在车载系统上的优势
	* 8155上通过虚拟化，qnx支持功能安全功能，android支持座舱娱乐等非功能安全功能
* orin 平台：
	个人感觉，是车载soc中性能独一档的存在，包括但不限于：cpu、npu算力强、内存大、外设丰富、开发环境无限接近普通linux，甚至支持图形化系统、第三方支持和nvidia生态完善（体现在外设接入、算法适配上）
* 地平线平台：
	克制化，专用算力（bpu）
	上手有一定成本，主要包括 vio、bpu两打自研sdk研究
* tda4：
	同样客制化，需要多核异构的开发能力，原生的TIVX框架需要掌握（基于共享内存，多核通信），C71、Dsp使用起来有难度


##### 职责：
 参与部分模块开发与维护（都不是从0负责）
	* 相机服务，平台适配为主
	* 图像拼接模块，平台适配为主
	* AI模型推理模块，开发+平台适配
	* 图像描画模块，开发
	* 特殊平台的适配工作，如J3行泊一体，负责相机切换部分开发；TDA4 泊车，适配封装其他厂商（freetech）中间件


#### 相机服务：
##### 主要功能：
接收相机传感器数据，并封装成统一的数据结构，传输给后续模块

##### 技术点：
* 新平台的了解：
	地平线有自定义的图像处理组件 J3 VIO、J5 VPS、ORIN直接使用通用的图像采集框架 Video4Linux (V4L2)、TDA4上需要与其他开发商的中间件间接获取
* 部分平台涉及相机通道的复用
	此前J3上由于VIO性能问题，同时搭载泊车、行车软件时，涉及VIO通道复用；需要额外增加行泊切换处理


#### 图像拼接

##### 职责：
后期开发和部分平台适配
##### 主要功能：
通过标定生成的像素映射表（LUT表），对图像进行 去畸变&拼接，为算法模块和描画模块提供有效的图像数据

##### 技术点：
* 去畸变和拼接基于LUT表，即原图像素->目标像素的一一映射；部分情况在映射的过程中还需添加图像格式转换，如 yuyv原图->rgb拼接图
* 平台客制化，由于不同平台模型的输入格式不同
* 数据缓存，维护多个图像队列，这是由于不同模型的输入不一致：pld、seg依赖bev图，od依赖柱状图（仅y轴去畸变）


#### 模型部署工作

##### 职责：
与算法同事合作，算法同事提供PC验证完成的模型（tensorflow、pytorch）基于平台提供的sdk转化为嵌入式版本

##### 技术点
* 通用代码设计，尽可能保证多种平台代码复用：
	对模型参数、前处理、运算、后处理等环节进行数据和方法抽象；
* 平台客制化。npu运算方式不同（同步方式：阻塞，异步方式：回调、信号同步），内存使用不同（共享内存、自定内存、物理虚拟内存映射等等）；提升运算效率，减少内存拷贝
* 部分场景需要排流水（J3 双bpu），大部分情况，npu会自动排流水

#### 描画开发
* 偏向体力活，理解上的难点在于拼接算法和描画算法（前人遗留，使用就好），拼接是基于LUT表的，LUT表定义了原图和bev图像素的一一对应关系
* 重点是提高内存利用率&运算效率


### 系统软件

	* 软件适配：如相机、网卡适配新外设（涉及代码修改），内存、emmc配置修改
	* 编译打包：软件编译客制化，与其他应用打包，支持ota本地升级

* 工具链开发：
	* 负载监控程序，用于集成软件的压测（包括）

* 集成工作：
	包括：
	* 整体编译，主要工作在jenkins脚本配置方面；并以整体软件包的形式发布
	* 集成问题排查：资源问题和程序异常问题

##### 软件集成问题解决：

##### 挂死问题：
* 首先关注程序挂死时收到的信号，程序挂死大都伴随着信号的生成，例外场景：进程进入异常判断并return等；进程信号通过前台运行打印，系统日志（kill by sig...），core文件等体现
* 通常的SIG类型：
	* sigsegv（段错误）：大多是进程bug导致，段错误代表着访问了越界内存、非法内存等；
	* sigabrt：代标触发了 assert等异常判断，软件abort（例：lock了一个已被lock了的mutex，触发assert，涉及到pthread_mutex_lock非原子）
	* sigbus：比较少见、硬件异常，或者访问了无效内存（如非对齐内存）等；也要可能是栈越界破坏了栈内容触发
	* sigusr，收到未定义的用户信号，进程会退出；例：此前J3上，地平线系统服务会无法sigusr，bug，进程会被误杀，需要认为添加默认处理
	* sigkill，认为杀死，或系统杀死；最常见的就是 OOM，dmseg里会有打印
	* sigstop（ctrl+z）、sigkill(-9)、sigint（ctrl+C），通常是认为停止，其中sigstop 遇到过误触发，就是某些版本gdb会误接收sigstop导致调试中的进程退出
* 首先确认是否是外部杀死：
	* sigkill，sigint等
	* dmesg看到打印信息，killed by...，如 OOM是进程被杀死会有信息
* 其次确认内部原因导致挂死：
	最常见的就是sigseg（segement fault），往往通过bt就能查到挂死的代码段，并进行分析
	几个例子：
	* free空指针
	* write非法fd
* 栈溢出问题：
![[Pasted image 20250327155109.png]]
	* 常见的原因，是访问栈上变量出现了越界；有时越界仅破坏其他变量，例如篡改了其他指针导致访问时出错，比较好排查；有时越界会破坏栈帧等关键栈结构，比较难排查，会导致bt显示异常，甚至出现问号
	* 排查思路：
		* 能查到挂死位置的，gdb查看当前变量情况，推测被哪个变量踩了，例如：变量变为了0，是前置变量memset 0 越界导致的
		* 问号，无法查到挂死位置，首先尝试从非问号部分；另外可以分析历史栈，栈调用如上图，通过找到合理的<LR,FP>调用链，找到LR所指向的代码段

* 堆内存越界引发的随机错误：
	最难查的问题，特别是内存在堆上时，分布是随机的
	静态检查、通过开关组件进行筛查缩小范围
	钩函数、preload 修改 alloc，查看内存使用情况，适合查内存泄露
	（有越界检查工具，没用过，没有嵌入式版本，所以当时不会弄）	

* 进程正常退出，系统会释放所有资源吗
	会，但需要过程，例如未被delete的内存，异常退出时为被回收的fd，系统会标记为待回收，但需要时间，期间可能会产生问题


* core bt为问号的原因：
	* 未携带debug信息，查看是否带有stripped，stripped代表舍去调试信息
	* 程序版本，gdb环境与板端不一致；
	* gdb库不全需要手动加载
	* gdb和编译时gcc版本不一致
	* 栈溢出导致的坏栈
	问号时解决方案：
	* 调试gdb环境
	* 通过info line 、disassemble等直接加载代码段信息和汇编信息
* 资源溢出问题：内存（勾函数 、preload），cpu超载（htop、pidstat，通过thread id检索问题线程）、fd溢出（lsof查看，strace跟踪异常fd）、IO阻塞（iostat查看，通常会造成进程卡顿）
* 通信问题：
* eth：抓包最为直接，判断网路问题在系统内/系统外，驱动层/应用层
* spi等，需要依赖底软，结合底软log排查
* 


#### 跳槽原因：
* 公司运营出现问题，薪资拖欠
* 近期缺少好的项目机会，目前几乎都是重复向的工作，如新平台适配，旧软件维护，缺乏发展机会
* 公司业务线有限，没有新的挑战，想开阔下眼界

#### 当前工作：
* 平台J6 APA移植，主要是模型和前处理适配
* 外部演示demo，负责搭建3ORIN+switch的分布式平台，展示sensor的快速接入和交换机的CB功能
* 中间件项目的服务支持：时间同步、时钟服务

#### 个人提问：
* 主要业务方向，有没有完整且长期的业务线（不想做零散的任务）
* 内部合作为主，还是与外部供应商合作为主；偏向自研还是偏向集成外部组件

HR 提问：
* 薪资组成，是否有绩效占比
* 是否有稳定的调薪或晋升机会
* 试用期是否支持请假，会有不好的影响吗（端午节要请3天）