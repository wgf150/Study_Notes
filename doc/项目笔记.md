
# 芯驰X9SP

### SDNN

#### 环境部署

![[1.png]]
图示为整套sdnn的开发框架
自下往上：
* 蓝色部分：模型的编译和仿真（docker容器中中完成）
* 橙色部分：模型的板端测试和benchmark（sdnn run 工具）
* 浅绿色部分：芯驰提供的模型和应用开发示例（model zoo和examples）
* 深绿色部分：将不同框架的模型文件通过编译工具链转换为SDNN格式文件，即tar格式（sdnn build）

大致工作流程如下：
![[2.png]]


##### 开发机：
docker部署部分，见 调试笔记 docker
docker中需要安装sdnn工具
```bash
#安装命令
pip install sdnn_release/sdnn-3.2.1-cp38-cp38-linux_x86_64.whl
```
##### 目标机:
部署runtime 库，需要与sdnn工具匹配




### examples

前置部分需要再docker中安装sdnn工具，否则构建环境变量时会报错 

```bash
#配置环境变量
source envsetup.sh
```


### sdnn工具

sdnn工具主要承载了两个功能：模型编译和模型推理
使用如下参数（必要参数）
```
#使能模型编译工具
sdnn build
#使能模型推理工具
sdnn run
```


### sdnn 编译

#### 1. 直接使用cmake编译，发现默认编译为x86 linux版本

![[3.png]]

examples中build_app.sh中会有额外的脚本，用于配置不同平台的编译参数
故后续参照build_app.sh构建脚本

![[4.png]]

#### 2. 编译qnx版本，显示libopencv_dnn.so格式不匹配

![[5.png]]

其中使用/usr/local/lib/libopencv_dnn.so，通过file查看，为x86_linux版本，怀疑存在参数未配置

![[6.png]]

发现在utils.cmake中，为给OpenCV设置qnx下的opencv参数，并且qnx下依赖opencv的是sdnn_sdk下的部分源码（据说为slimAI相关，可以不需要），所以把这两部分删除
* 修改CMakeLists.txt
![[7.png]]
* 修改utils.cmake
![[8.png]]


#### 3. libtvm_runtime.so存在库依赖，docker中找不到

消除opencv问题后，发现仍然存在编译问题
![[9.png]]

这三个库docker中不存在，但板端存在，readelf发现确实与libtvm_runtime.so间存在依赖关系

![[10.png]]




#### 问题：
* 模型基于什么运行，cfg？tar？so？examples中模型基于.so，文档中基于cfg，实际生成为tar
* qnx中是否支持openvx




# KBD gptp测试

preload测试：
* 调频失败   
	int clock_adjtime(clockid_t id, struct timex *tx);
* 发送失败
	ssize_t send (int __fd, const void *__buf, size_t __n, int __flags)；







# Fii TDA4 vision_app

## dl_demos

```C
app_tidl_main
└ app_parse_cmd_line_args          // 解析参数
	└ app_set_cfg_default          // 默认配置
	└ app_parse_cfg_file           // 解析参数中的配置文件（代替默认配置） 
└ app_init
	└ vxCreateContext              // （sdk）创建context
	└ readConfig                   // 生成config（vx_user_data_object）
		└ vxCreateUserDataObject   // 创建，申请空间
		└ vxMapUserDataObject      // 空间mapping -> tidlParams	
		└ vxUnmapUserDataObject    // unmaping
	└ readNetwork                  // 生成network（vx_user_data_object）
		└ vxCreateUserDataObject
		└ vxMapUserDataObject      // 空间mapping -> network_buffer
	└ SetCreateParams              // 与上同理，生成createParams
	└ setInargs                    // 与上同理，生成inArgs
	└ setOutArgs                   // 与上同理，生成outArgs
	└ 
└ app_create_graph
	└ vxCreateGraph                //
	└ vxSetReferenceName
	└ createInputTensors
	└ createOutputTensors
	└ initParam
	└ addParam                     // 依次添加Param
	└
	└
└ app_verify_graph
└ app_run_graph
```

* vx_user_data_object结构
	
* readConfig （tidl_config_file_path），文件内容即sTIDL_IOBufDesc_t结构体，size需完全一致；readNetwork（tidl_network_file_path） 


## aiserver集成

关注点：
* 模型输入格式，dl_demo中为bmp读入，需修改为ai_server输入格式



readInput()：
* obj->ioBufDesc在readConfig()中初始化，从conf文件中读取。
![[dl_demo_1.png]]
* tensor使用前后需要 map&unmap
```C
if(status == VX_SUCCESS)
        {
            status = tivxMapTensorPatch(input_tensors[id], 3, start, input_sizes, &map_id_input, input_strides, &input_buffer, VX_WRITE_ONLY, VX_MEMORY_TYPE_HOST);
        }

...

tivxUnmapTensorPatch(input_tensors[id], map_id_input);

```


# J6模型转换

### 环境准备
硬件方面
选用天准 J6M开发板
上电：

几种连接方式：
* 串口：选用usb2口 chC（共有chA-chD四个channel）
* 网口：

软件方面
主要工具链有两个：
* J6 全量开发包（OE包），
* J6 dokcer 开发环境（cpu/gpu）
![[J6模型转换.png]]

避免复杂的环境安装，这里优先选择了docker环境（cpu版本）
使用docker只有两步：
* 加载 docker 镜像
* 启动 docker 容器
```bash
# 1
sudo docker load -i docker_openexplorer_xxx.tar.gz

# 2 
# 官方脚本
sh run_docker.sh data/ cpu
# 也可以自定义
sudo docker run -it --rm -v /home/wangguofan/J6/horizon_j6_open_explorer_v3.0.22-py310_20240924:/open_explorer -v /home/wangguofan/J6/data:/data/horizon_j6/data  openexplorer/ai_toolchain_ubuntu_22_j6_cpu:v3.0.22
```

### 模型转换
这里采用PTQ量化方法，即使用一批校准数据对训练好的模型进行校准，此处以 pld 模型转换为例

#### 1. 模型验证
```bash
hb_compile --model pld_20240417.onnx --march nash-m
```
* --model 指定原始模型
* --march 指定适配的处理器类型，J6E 为 nash-e，J6M 为 nash-m

如下，终端无报错，则认为验证完成
![[J6模型转换_2.png]]


#### 2. 校准数据预处理

***校准数据*** 是从训练集或验证集中筛选的一定数量的典型数据，并且避免非常少见的异常样本，如纯色图片、不含任何检测或分类目标的图片等；筛选出的校准数据还需进行与模型inference前一致的预处理操作，处理后保持与原始模型一样的数据类型（ input_type_train ）、layout （ input_layout_train ）和尺寸（ input_shape ）。

地平线提供了一系列官方的脚本，具体调用顺序：
02_preprocess.sh（客制化）
 └─> data_preprocess.py（通用）
	  └─>preprocess.py（客制化）
		   └─> transform.py（通用）
所以这里主要关注客制化的部分：
* preprocess.py ：利用transform.py提供的基本转换函数，定义实际模型需要的转换函数```calibration_transformers()```；
	这里可以引用以pld为例，具体的转换步骤如下：
```python
import numpy as np

from horizon_tc_ui.data.dataloader import (ImageNetDataLoader,
                                           SingleImageDataLoader)
# yapf: disable
from horizon_tc_ui.data.transformer import (HWC2CHWTransformer,
                                            MeanTransformer,
                                            PILCenterCropTransformer,
                                            PILResizeTransformer,
                                            RGB2NV12Transformer,
                                            ScaleTransformer)

# yapf: enable

short_size = 768
crop_size = 768


def calibration_transformers():
    transformers = [
	    # 等比例缩小至短边为 768
        PILResizeTransformer(size=short_size),
        # 截取中心768*768图像
        PILCenterCropTransformer(size=crop_size),
        # BGR 转 BGR-planer
        HWC2CHWTransformer(),
        # 量化步骤1：像素减去mean_value
        MeanTransformer(means=np.array([127.5, 127.5, 127.5])),
        # 量化步骤2：像素乘以data_scale
        ScaleTransformer(
            scale_value=np.array([0.01307190, 0.01307190, 0.01307190]))
    ]
    return transformers
```

* 02_preprocess.sh：调用data_preprocess.py的具体参数
	注：data_preprocess.py 会在当前目录下查找 preprocess.py，所以要确保运行路径下放有正确的preprocess.py
	同样以pld为例，脚本内容如下：
	* src_dir：原始校准数据路径
	* dst_dir：处理后的数据存储路径
	* pic_ext：处理后的数据文件后缀（可不配）
	* read_mode：图片读取方式，这里选用opencv，默认读取为BGR，数据范围为0~255
	* saved_data_type：处理后的数据保存类型
```python
# 这里名称为 pld_preprocess.sh
python3 ../../../data_preprocess.py \
  --src_dir ./calib_data \
  --dst_dir ./calib_data_npy \
  --pic_ext .bgr \
  --read_mode opencv \
  --saved_data_type float32
```

脚本配置完成后，可以运行pld_preprocess.sh来生成校准数据，最终结果如下：
![[J6模型转换_3.png]]


#### 3. 模型转换

转换通过```hb_compile```工具进行，依赖一份yaml配置文件，其中必选数组为：`model_parameters` 、 `input_parameters` 、 `calibration_parameters` 、 `compiler_parameters`。

以pld为例，最终使用的配置为：
```yaml
model_parameters:
  # The model file of floating-point ONNX neural network data
  onnx_model: './pld_20240417.onnx'
  # BPU architecture, with range: nash-e/nash-m
  march: "nash-m"
  # Specify the directory to store the model conversion output
  working_dir: 'model_output'
  # Specify the name prefix for output model
  output_model_file_prefix: 'pld_20240417'

input_parameters:
  # Specify the input node name of the original floating-point model,
  # consistent with its name in the model, not specified will be obtained from the model
  input_name: "data"
  # Specify the input data of the on-board model, with range: nv12/rgb/bgr/yuv444/gray/featuremap
  input_type_rt: 'bgr'
  # Specify the input data type of the original floating-point model, with range: rgb/bgr/gray/yuv444/featuremap
  # The number/order specified need to be the same as in input_name
  input_type_train: 'bgr'
  # Specify the input data layout of the original floating-point model, with range: NHWC/NCHW
  # The number/order specified need to be the same as in input_name
  input_layout_train: 'NCHW'
  # Specify the input shape of the original floating-point model, e.g. 1x3x224x224;1x2x224x224
  input_shape: '1x3x768x768'
  # Specify the batch of the on-board model
  # Only supported for single-input model and the input's first dimension is 1.
  #input_batch: 1
  # Specify the model input preprocessing method, with range: no_preprocess/data_mean_and_scale/data_mean/data_scale
  norm_type: 'data_mean_and_scale'
  # Specify the mean value to be subtracted by the preprocessing input image
  # For channel mean values, use space or ';' for separation
  mean_value: "127.5 127.5 127.5"
  # Specify the scale factor of the preprocessing input image
  # For channel scale, use space or ';' for separation
  scale_value: "0.01307190 0.01307190 0.01307190"

calibration_parameters:
  # Specify the directory to store the calibration data, support Jpeg, Bmp etc, which should cover the typical scenarios
  # For multiple inputs, need to set multiple directories, and use ';' for separation
  cal_data_dir: './calib_data_npy'
  # Specify the type of algorithm used for calibration, with range: default/mix/kl/max/load
  # Select skip for only performance verification, select load when using the QAT exported model
  # Usually default is enough to meet the requirements, if not, try in the order of mix, kl/max
  calibration_type: 'default'

# Compilation related parameters
compiler_parameters:
  # Specify the model compilation optimization level
  # O0 means no optimization, O1-O3 means more optimization as the level increases, but higher compilation time
  optimize_level: 'O0'
```

##### 格式选择问题：
* input_type_rt支持的格式中，默认都是NHWC（packed）存储的，例如：在input_shape 为 1×3×768×768（NCHW） 的情况下，模型转换后，输入格式变为了 1×768×768×3（NHWC）
![[J6模型转换_5.png]]
* 使用 hbUCPMalloc 与 hbUCPMallocCached 接口申请的内存首地址默认 32 对齐，实测总结，除了描述字长的stride，其余stride需要满足 4 Bytes对齐
![[J6模型转换_6.png]]
如上，1×768×768×3 对齐后的分布为 1×768×768×4

综合上述两点，在面对 APA 模型 768×768×3 bgr 的输入时，不可避免产生对齐问题，目前想到的三种解决方法：
1. input_type_rt 选择NV12，由于NV12排布的特殊性，仅要求W为32位对齐即可；
2. input_type_rt 选择bgr，但实际输入bgra，a不生效但充当padding作用，在拼接时添加padding
3. input_type_rt 选择bgr，在最终输入模型时，添加padding
（方法2 最优，不过需要开发，在方法3的基础上，节省了ai_server中添加padding的代码）

#### 4. C++API

#####  编译问题：
bpu相关编译时，动态库多存在多重依赖，会报以下warning
![[J6模型转换_4.png]]
链接问题解决：https://zhuanlan.zhihu.com/p/559612163
或者用-Wl,-unresolved-symbols=ignore-in-shared-libs 参数忽略warning

注：warning是链接器ld链接时报错，故无法使用-L、link_dicrectries等解决，因为它们解决的是编译器编译时的符号缺失；
* -rpath-link：仅作为 链接时库查找路径
* -rpath：作为 链接及程序运行时的库查找路径

##### buffer对齐问题：

hbDNNTensor.properties.alignedByteSize记录了对齐后的内存占用大小。这里要求各stride必须是偶数，所以实际通过hbUCPMallocCached申请的大小是 4\*768\*768；可以通过hbDNNTensor.properties.validShape获取tensor的有效尺寸
```cpp
for (int i = 0; i < output_count; i++) {        
        hbDNNGetOutputTensorProperties(&output_tensors_[i].properties, dnn_handle_, i);

        ret = hbUCPMallocCached(&output_tensors_[i].sysMem, output_tensors_[i].properties.alignedByteSize, 0);
    } 
```
APA中，有效数据内容为 3\*768\*768，对齐后，数据仍然连续，所以可以通过memcpy连续拷贝

##### cache使用

BPU运算使用CACHE提供访存效率，所以在读写tensor时，需要手动刷新CACHE：
```cpp
typedef enum {
  HB_SYS_MEM_CACHE_INVALIDATE = 1,
  HB_SYS_MEM_CACHE_CLEAN = 2
} hbUCPSysMemFlushFlag;
```
* HB_SYS_MEM_CACHE_INVALIDATE：将内存同步到缓存中，CPU读前使用。
* HB_SYS_MEM_CACHE_CLEAN：将缓存数据同步到内存中，CPU写后使用。
使用 hbUCPMemFlush 进行刷新

##### UCP 统一运算平台

地平线定义的一套统一的异构编程接口，实现对计算平台上所有资源的调用。 UCP将SOC上的功能硬件抽象出来并进行封装，对外提供基于功能的API，用于创建相应的UCP任务（如VP算子任务），并支持设置硬件Backend提交至UCP调度器， UCP可基于硬件资源，完成SOC上任务的统一调度。

以bpu为例，backend包括
```cpp
#define HB_UCP_BPU_CORE_0 (1ULL << 0)
#define HB_UCP_BPU_CORE_1 (1ULL << 1)
#define HB_UCP_BPU_CORE_2 (1ULL << 2)
#define HB_UCP_BPU_CORE_3 (1ULL << 3)
#define HB_UCP_BPU_CORE_ANY (1ULL << 7)
```
可以通过 hbUCPWaitTaskDone 来监控BPU推理完成：
```cpp
	hbUCPSchedParam ctrl_param;
    HB_UCP_INITIALIZE_SCHED_PARAM(&ctrl_param);
    ctrl_param.backend = HB_UCP_BPU_CORE_ANY;
    hbUCPSubmitTask(task_handle, &ctrl_param);
    
    hbUCPWaitTaskDone(task_handle, 0);

    hbUCPReleaseTask(task_handle);
```